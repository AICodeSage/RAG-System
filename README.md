# Advanced RAG System

A production-grade Retrieval-Augmented Generation system with hybrid search, conversation memory, and confidence scoring.

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ” **Hybrid Search** | Combines dense embeddings (sentence-transformers) + sparse BM25 with Reciprocal Rank Fusion |
| ğŸ§  **Query Enhancement** | LLM-powered query rewriting and expansion for better recall |
| ğŸ’¬ **Conversation Memory** | Multi-turn chat with context awareness |
| ğŸ“š **Citations** | Inline source references [1], [2], etc. |
| ğŸ“Š **Confidence Scoring** | Visual indicators (ğŸŸ¢ğŸŸ¡ğŸ”´) showing answer reliability |
| âš¡ **Streaming** | Real-time response generation |
| ğŸ—‚ï¸ **Multi-format** | Supports PDF, TXT, MD, JSON, CSV, RST |

## ğŸš€ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Add your OpenAI key to .env
echo "OPENAI_API_KEY=sk-..." > .env

# Add documents to uploads/
cp your_docs.pdf uploads/

# Run
python3 main.py --use-llm --docs-dir uploads
```

## ğŸ“– Usage

```
============================================================
ğŸ” Advanced RAG System
   12 chunks indexed
   Embedding dimension: 384
   Hybrid search: Dense + BM25
------------------------------------------------------------
Type your question and press Enter.
Type /help for commands, or quit to exit.
============================================================

You: What is MediRescue?

ğŸ¤– Assistant: MediRescue is an AI-powered micro-health coverage platform 
designed to make emergency healthcare affordable [1]. It offers flexible 
micro-payments starting at R20/month and includes features like AI-powered 
triage and medicine vouchers [2].

You: What technology stack does it use?

ğŸ¤– Assistant: Based on the documentation, MediRescue uses:
- Frontend: Next.js with Vercel AI SDK [1]
- Backend: Python with Agno agents [2]
- Database: PostgreSQL [1]

You: /sources

ğŸ“š Sources:
  [1] MediRescue_Documentation
      Score: 0.847
      Snippet: MediRescue â€“ Intelligent Micro-Health Coverage Platform...
  [2] MediRescue_Documentation
      Score: 0.723
      Snippet: System Architecture Frontend: Next.js + Vercel AI SDK...

You: /debug

âœ“ Debug mode: ON

You: quit
ğŸ‘‹ Goodbye!
```

## ğŸ® Commands

| Command | Description |
|---------|-------------|
| `/help` | Show available commands |
| `/clear` | Clear conversation history |
| `/sources` | Show sources from last answer |
| `/debug` | Toggle debug mode (shows confidence, retrieval scores) |
| `quit`, `exit`, `q` | Exit the system |

## ğŸ› ï¸ CLI Options

```bash
python3 main.py [OPTIONS]
```

| Flag | Description |
|------|-------------|
| `-d`, `--docs-dir` | Path to documents (default: `uploads`) |
| `--use-llm` | Use OpenAI for answer generation |
| `--openai-embeddings` | Use OpenAI embeddings instead of sentence-transformers |
| `--show-steps` | Show detailed processing logs |
| `--stream` | Stream responses in real-time |
| `--no-enhancement` | Disable query enhancement |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INDEXING PIPELINE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PDF/TXT â†’ Semantic Chunking â†’ Embeddings â†’ FAISS + BM25       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       QUERY PIPELINE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Query â†’ Enhancement â†’ Hybrid Search â†’ MMR Rerank â†’ Answer     â”‚
â”‚           â†“              â†“               â†“           â†“         â”‚
â”‚       Rewriting    Dense+BM25+RRF    Diversity   Citations     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

```
rag-system/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ embeddings.py    # Sentence-transformers / OpenAI
â”‚   â”œâ”€â”€ index.py         # FAISS vector store
â”‚   â”œâ”€â”€ memory.py        # Conversation memory
â”‚   â”œâ”€â”€ document.py      # Document model
â”‚   â”œâ”€â”€ chunk.py         # Chunk model
â”‚   â””â”€â”€ context.py       # Context builder
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ loaders.py       # PDF, text, markdown loaders
â”œâ”€â”€ chunking/
â”‚   â””â”€â”€ semantic.py      # Sentence/paragraph chunking
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ hybrid.py        # Dense + BM25 with RRF
â”‚   â”œâ”€â”€ query.py         # Query enhancement (HyDE, expansion)
â”‚   â””â”€â”€ mmr.py           # MMR reranking
â”œâ”€â”€ generation/
â”‚   â”œâ”€â”€ answer.py        # Answer with citations + confidence
â”‚   â”œâ”€â”€ llm.py           # OpenAI integration
â”‚   â””â”€â”€ generator.py     # Local fallback
â””â”€â”€ main.py              # Interactive CLI
```

## ğŸ”¬ How It Works

### 1. Hybrid Search
Combines two retrieval methods:
- **Dense**: Sentence-transformer embeddings (384-dim) with FAISS
- **Sparse**: BM25 keyword matching

Results are merged using **Reciprocal Rank Fusion (RRF)**:
```
score = Î± Ã— (1/(k + dense_rank)) + (1-Î±) Ã— (1/(k + sparse_rank))
```

### 2. Query Enhancement
Uses LLM to improve queries:
- **Rewriting**: Clarifies ambiguous queries
- **Expansion**: Adds synonyms and related terms
- **HyDE**: Generates hypothetical answers for embedding

### 3. Confidence Scoring
Calculates answer reliability based on:
- Max retrieval score
- Average retrieval score
- Score threshold comparison

Visual indicators:
- ğŸŸ¢ High (â‰¥70%): Reliable answer
- ğŸŸ¡ Medium (40-70%): Possible gaps
- ğŸ”´ Low (<40%): Uncertain, verify independently

### 4. Conversation Memory
Maintains context across turns:
- Sliding window of recent messages
- Context injection for query enhancement
- Source tracking across conversation

## ğŸ“Š Performance Tips

1. **Use PyMuPDF** for PDFs: `pip install pymupdf`
2. **Use FAISS**: Already included for fast vector search
3. **Tune alpha** in hybrid search (0.6 favors dense, 0.4 favors keywords)
4. **Chunk size**: 512 chars works well for most documents

## ğŸ”’ Security

- API keys stored in `.env` (gitignored)
- No secrets in commit history
- Rotate keys if exposed

DIMPHO KGAUME PITSIğŸ«†


## ğŸ“ License

MIT
